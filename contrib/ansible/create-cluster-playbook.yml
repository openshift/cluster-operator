#!/usr/bin/ansible-playbook
#
# Create a test Cluster in the Cluster Operator.
#
---
- hosts: localhost
  connection: local
  gather_facts: no

  # Variables you can optionally override from the CLI:
  vars:
    # NOTE: you may set 'cluster_namespace' to control where the cluster object is created. We do not
    # set a default here as we need to lookup the current one if unset.

    # ID for your cluster, defaults to current username:
    cluster_name: "{{ lookup ('env', 'USER') }}"

    # ClusterVersion to use
    cluster_version: origin-v3-10

    # Namespace where we expect cluster version to exist, does not have to match the cluster namespace
    # and generally this can be left as is:
    cluster_version_namespace: openshift-cluster-operator

    # Location of AWS credentials file:
    aws_creds_file: ~/.aws/credentials

    # Section in credentials file with the AWS creds CO should use to create cloud infra and VMs:
    aws_creds_section: default

    # SSH private key used for access to all VMs CO creates:
    ssh_priv_key: '~/.ssh/libra.pem'

    # Force regeneration of cluster cert. Will happen regardless if one does not exist.
    redeploy_cluster_cert: False

    # Paths where we will generate self-signed certs for the specific cluster. (probably don't need to
    # override these but if you wanted to use pre-existing certs you could)
    cluster_cert_dir: "{{ playbook_dir }}/../../certs"

    # This CA will be generated once and then re-used for all certs. Will be created
    # in the cert dir defined above.
    cluster_ca_name: ca

    # Allow overriding the default of having an S3-backed cluster registry
    s3_backed_registry: "true"

  tasks:

  - fail:
      msg: "Please specify a template file. See contrib/examples/cluster-deployment-template.yaml for an example."
    when: cli_template_file is undefined

  - import_role:
      name: kubectl-ansible

  # If no cluster_namespace was defined on the CLI, we want to create the cluster in the current:
  - name: lookup current namespace if none defined
    command: "oc project -q"
    register: current_namespace_reg
    when: cluster_namespace is not defined

  - set_fact:
      cluster_namespace: "{{ current_namespace_reg.stdout }}"
    when: cluster_namespace is not defined

  # Command will error if it does not exist. Technically this is fine, you can create the cluster
  # and wait for the version to exist, but in this case it probably signifies a mistake by the user.
  - name: verify cluster version exists
    command: "oc get clusterversion {{ cluster_version }} -n {{ cluster_version_namespace }}"
    changed_when: false

  - name: create cluster namespace
    kubectl_apply:
      definition:
        kind: "Namespace"
        apiVersion: v1
        metadata:
          name: "{{ cluster_namespace }}"

  - name: check for password-protected ssh key
    command: "grep ENCRYPTED {{ ssh_priv_key }}"
    ignore_errors: yes
    failed_when: false
    changed_when: no
    register: pass_protect_ssh

  - fail:
      msg: password protected ssh key not supported
    when: pass_protect_ssh.rc == 0

  - name: ensure certs directory exists
    file:
      path: "{{ playbook_dir }}/../../certs/"
      state: directory

  - include_role:
      name: cluster_operator_certgen
    vars:
      certgen_cert_dir: "{{ cluster_cert_dir }}"
      certgen_ca_name: "{{ cluster_ca_name }}"
      certgen_cert_name: "{{ cluster_name }}"
      # TODO: do something meaningful here?
      certgen_cn: "{{ cluster_name }}"
      certgen_alt_names: "\"{{ cluster_name }}.{{ cluster_namespace }}\",\"{{ cluster_name }}.{{ cluster_namespace }}.svc\""

  # These are expected locations of the files given the directory and names provided when
  # we called the role.
  - name: set implicit cert path facts
    set_fact:
      cluster_cert_path: "{{ cluster_cert_dir }}/{{ cluster_name }}.pem"
      cluster_privatekey_path: "{{ cluster_cert_dir }}/{{ cluster_name }}-key.pem"

  - name: generate ssh public key from ssh private key
    command: "ssh-keygen -y -f {{ ssh_priv_key }}"
    register: ssh_keygen_output

  - name: load cluster certs, keys and credentials
    set_fact:
      # base-64-encoded, pem cert for the cluster's ELB and rounter pods:
      l_cluster_cert: "{{ lookup('file', cluster_cert_path) | b64encode }}"
      # base-64-encoded, pem private key for the cluster's cert:
      l_cluster_cert_key: "{{ lookup('file', cluster_privatekey_path) | b64encode }}"
      l_aws_access_key_id: "{{ lookup('ini', 'aws_access_key_id section=' + aws_creds_section + ' file=' + aws_creds_file) | b64encode }}"
      l_aws_secret_access_key: "{{ lookup('ini', 'aws_secret_access_key section=' + aws_creds_section + ' file=' + aws_creds_file) | b64encode }}"
      l_aws_ssh_private_key: "{{ lookup('file', ssh_priv_key) | b64encode }}"
      l_aws_ssh_public_key: "{{ ssh_keygen_output.stdout | trim | b64encode }}"

  - name: process cluster template
    oc_process:
      template_file: "{{ cli_template_file }}"
      parameters:
        CLUSTER_NAME: "{{ cluster_name }}"
        CLUSTER_NAMESPACE: "{{ cluster_namespace }}"
        CLUSTER_VERSION: "{{ cluster_version }}"
        CLUSTER_VERSION_NAMESPACE: "{{ cluster_version_namespace }}"
        CLUSTER_CERT: "{{ l_cluster_cert }}"
        CLUSTER_PRIVATE_KEY: "{{ l_cluster_cert_key }}"
        AWS_ACCESS_KEY_ID: "{{ l_aws_access_key_id }}"
        AWS_SECRET_ACCESS_KEY: "{{ l_aws_secret_access_key }}"
        SSH_PRIVATE_KEY: "{{ l_aws_ssh_private_key }}"
        SSH_PUBLIC_KEY: "{{ l_aws_ssh_public_key }}"
        S3_BACKED_REGISTRY: "{{ s3_backed_registry }}"
    register: cluster_reg

  - name: create/update cluster
    kubectl_apply:
      definition: "{{ cluster_reg.result | to_json }}"


